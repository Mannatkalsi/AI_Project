{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "# using an alternative backend to macos gui driver, because there is an issue with\n",
    "# matplotlib, virtualenv and macos: https://github.com/pypa/virtualenv/issues/54\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "The Driver class marshals the behaviour of the provided gym environment and\n",
    "agent to train, update and evaluate it's ability to perform.\n",
    "\n",
    "Driver contains the main training loop and can plot charts to show training\n",
    "performance, as well as use the environments render method to demonstrate\n",
    "trained performance.\n",
    "\n",
    "Driver contains specific methods to arrange running for combinations of\n",
    "agent and environment.\n",
    "\"\"\"\n",
    "class Driver:\n",
    "    def __init__(self, params):\n",
    "        self.epochs = params['epochs']\n",
    "        self.env = params['env']\n",
    "        self.agent = params['agent']\n",
    "        self.training_rewards = []\n",
    "        self.evaluation_rewards = []\n",
    "\n",
    "    def run_taxi_random(self):\n",
    "        training_action = lambda _observation: self.agent.action(self.env)\n",
    "        update = lambda _observation, _action, _reward: None\n",
    "        evaluation_action = training_action\n",
    "\n",
    "        self.run(training_action, update, evaluation_action)\n",
    "\n",
    "    def run_taxi_qlearner(self):\n",
    "        self.agent.initialize_taxi_q_table(self.env)\n",
    "\n",
    "        training_action = lambda observation: self.agent.taxi_training_action(self.env, observation)\n",
    "        update = lambda observation, action, reward: self.agent.taxi_update(observation, action, reward)\n",
    "        evaluation_action = lambda observation: self.agent.taxi_evaluation_action(observation)\n",
    "\n",
    "        self.run(training_action, update, evaluation_action)\n",
    "\n",
    "    def run_cartpole_random(self):\n",
    "        training_action = lambda _observation: self.agent.action(self.env)\n",
    "        update = lambda _observation, _action, _reward: None\n",
    "        evaluation_action = training_action\n",
    "\n",
    "        self.run(training_action, update, evaluation_action)\n",
    "\n",
    "    def run_cartpole_qlearner(self):\n",
    "        self.agent.initialize_cartpole_q_table(self.env)\n",
    "\n",
    "        training_action = lambda observation: self.agent.cartpole_training_action(self.env, observation)\n",
    "        update = lambda observation, action, reward: self.agent.cartpole_update(observation, action, reward)\n",
    "        evaluation_action = lambda observation: self.agent.cartpole_evaluation_action(observation)\n",
    "\n",
    "        self.run(training_action, update, evaluation_action)\n",
    "\n",
    "    def run_cartpole_tdlearner(self):\n",
    "        self.agent.initialize_cartpole_q_policy(self.env)\n",
    "\n",
    "        training_action = lambda observation: self.agent.cartpole_training_action(self.env, observation)\n",
    "        update = lambda observation, action, reward: self.agent.cartpole_update(observation, action, reward)\n",
    "        evaluation_action = lambda observation: self.agent.cartpole_evaluation_action(observation)\n",
    "\n",
    "        self.run(training_action, update, evaluation_action)\n",
    "\n",
    "    def run_frozen_lake_random(self):\n",
    "        training_action = lambda _observation: self.agent.action(self.env)\n",
    "        update = lambda _observation, _action, _reward: None\n",
    "        evaluation_action = training_action\n",
    "\n",
    "        self.run(training_action, update, evaluation_action)\n",
    "\n",
    "    def run_frozen_lake_qlearner(self):\n",
    "        self.agent.initialize_frozen_lake_q_table(self.env)\n",
    "\n",
    "        training_action = lambda observation: self.agent.frozen_lake_training_action(self.env, observation)\n",
    "        update = lambda observation, action, reward: self.agent.frozen_lake_update(observation, action, reward)\n",
    "        evaluation_action = lambda observation: self.agent.frozen_lake_evaluation_action(observation)\n",
    "\n",
    "        self.run(training_action, update, evaluation_action)\n",
    "\n",
    "    def run_frozen_lake_tdlearner(self):\n",
    "        self.agent.initialize_frozen_lake_q_policy(self.env)\n",
    "\n",
    "        training_action = lambda observation: self.agent.frozen_lake_training_action(self.env, observation)\n",
    "        update = lambda observation, action, reward: self.agent.frozen_lake_update(observation, action, reward)\n",
    "        evaluation_action = lambda observation: self.agent.frozen_lake_evaluation_action(observation)\n",
    "\n",
    "        self.run(training_action, update, evaluation_action)\n",
    "\n",
    "    # main engine: training and evaluation loop, plot then demonstrate\n",
    "    def run(self, training_action, update, evaluation_action):\n",
    "        for i in range(self.epochs):\n",
    "            if ((i + 1) % 1000 == 0):\n",
    "                print(\"progress: {}%\".format(100 * (i + 1) // self.epochs))\n",
    "            self.train_once(training_action, update)\n",
    "            self.evaluate_once(evaluation_action)\n",
    "\n",
    "        self.plot()\n",
    "        \n",
    "        try:\n",
    "            self.demonstrate(evaluation_action)\n",
    "        except NotImplementedError:\n",
    "            print(\"Cannot demonstrate: render method on env not implemented.\")\n",
    "\n",
    "    # a single instance of training of the agent in the environment\n",
    "    def train_once(self, training_action, update):\n",
    "        observation = self.env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action = training_action(observation)\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            episode_reward += reward\n",
    "            update(observation, action, reward)\n",
    "        self.training_rewards.append(episode_reward)\n",
    "\n",
    "    # a single instance of evaluation of the agent at it's current level of training\n",
    "    def evaluate_once(self, evaluation_action):\n",
    "        observation = self.env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action = evaluation_action(observation)\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            episode_reward += reward\n",
    "        self.evaluation_rewards.append(episode_reward)\n",
    "\n",
    "    # plot training and evaluation reward levels at each epoch\n",
    "    def plot(self):\n",
    "        plt.subplot('211')\n",
    "        plt.plot(self.training_rewards, linewidth=1)\n",
    "        plt.title('Training reward over time')\n",
    "        plt.ylabel('reward')\n",
    "        plt.xlabel('iterations')\n",
    "\n",
    "        plt.subplot('212')\n",
    "        plt.plot(self.evaluation_rewards, linewidth=1)\n",
    "        plt.title('Evaluation reward over time')\n",
    "        plt.ylabel('reward')\n",
    "        plt.xlabel('iterations')\n",
    "        \n",
    "        plt.subplots_adjust(hspace=0.5)\n",
    "        plt.show()\n",
    "\n",
    "    # use the environments render method and print some additional info\n",
    "    # to the console. permit user input for repeated demonstrations in a loop\n",
    "    def demonstrate(self, evaluation_action):\n",
    "        observation = self.env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        reward = 0\n",
    "        step = 0\n",
    "        while not done:\n",
    "            print(f\"Step: {step} | Cumulative Reward: {episode_reward}\")\n",
    "            step += 1\n",
    "            print(\"RENDERING...\")\n",
    "            self.env.render()\n",
    "            action = evaluation_action(observation)\n",
    "            print('observation: ', observation)\n",
    "            print('action: ', action)\n",
    "            print('reward: ', reward)\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            episode_reward += reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
