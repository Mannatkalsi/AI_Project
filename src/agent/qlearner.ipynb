{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "#import numpy as np\n",
    "\n",
    "# cartpole bucket hyperparameters\n",
    "CARTPOLE_POSITION_BUCKETS = 2\n",
    "CARTPOLE_POSITION_RANGE = (-2.0, 2.0)\n",
    "CARTPOLE_VELOCITY_BUCKETS = 8\n",
    "CARTPOLE_VELOCITY_RANGE = (-1.2, 1.2)\n",
    "CARTPOLE_THETA_BUCKETS = 16\n",
    "CARTPOLE_THETA_RANGE = (-0.08, 0.08)\n",
    "CARTPOLE_THETA_VELOCITY_BUCKETS = 6\n",
    "CARTPOLE_THETA_VELOCITY_RANGE = (-1.2, 1.2)\n",
    "\n",
    "class Qlearner():\n",
    "    def __init__(self, parameters):\n",
    "        self.alpha = parameters['alpha']\n",
    "        self.gamma = parameters['gamma']\n",
    "        self.epsilon = parameters['epsilon']\n",
    "        super().__init__()\n",
    "\n",
    "    # TAXI\n",
    "\n",
    "    def initialize_taxi_q_table(self, env):\n",
    "        self.q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    def taxi_training_action(self, env, observation):\n",
    "        self.previous_observation = observation\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return env.action_space.sample() # explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[observation]) # exploit\n",
    "\n",
    "    def taxi_evaluation_action(self, observation):\n",
    "        return np.argmax(self.q_table[observation])\n",
    "\n",
    "    def taxi_update(self, observation, action, reward):\n",
    "        # updates the previous observation qtable entry with the reward gained,\n",
    "        # uses the maximum/best future option always\n",
    "        old_value = self.q_table[self.previous_observation, action]\n",
    "        next_max = np.max(self.q_table[observation])\n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n",
    "        self.q_table[self.previous_observation, action] = new_value\n",
    "\n",
    "    # CARTPOLE\n",
    "\n",
    "    def initialize_cartpole_q_table(self, env):\n",
    "        obs_space = CARTPOLE_POSITION_BUCKETS * CARTPOLE_VELOCITY_BUCKETS * CARTPOLE_THETA_BUCKETS * CARTPOLE_THETA_VELOCITY_BUCKETS\n",
    "        self.q_table = np.zeros([obs_space, env.action_space.n])\n",
    "\n",
    "        # establish weak priors to optimise training - if theta < 0, move left, if theta > 0 move right\n",
    "        for i in range(obs_space):\n",
    "            if (i % (CARTPOLE_THETA_BUCKETS * CARTPOLE_THETA_VELOCITY_BUCKETS) < (CARTPOLE_THETA_BUCKETS / 2)):\n",
    "                self.q_table[i][0] = 0.1\n",
    "            elif (i % (CARTPOLE_THETA_BUCKETS * CARTPOLE_THETA_VELOCITY_BUCKETS) >= (CARTPOLE_THETA_BUCKETS / 2)):\n",
    "                self.q_table[i][1] = 0.1\n",
    "\n",
    "    def cartpole_training_action(self, env, observation):\n",
    "        self.previous_observation = observation\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return env.action_space.sample() # explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[self._cartpole_obs_index(observation)]) # exploit\n",
    "\n",
    "    def cartpole_evaluation_action(self, observation):\n",
    "        return np.argmax(self.q_table[self._cartpole_obs_index(observation)])\n",
    "\n",
    "    def cartpole_update(self, observation, action, reward):\n",
    "        # updates the previous observation qtable entry with the reward gained,\n",
    "        # uses the maximum/best future option always\n",
    "        old_value = self.q_table[self._cartpole_obs_index(self.previous_observation), action]\n",
    "        next_max = np.max(self.q_table[self._cartpole_obs_index(observation)])\n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n",
    "        self.q_table[self._cartpole_obs_index(self.previous_observation), action] = new_value\n",
    "\n",
    "    def _cartpole_obs_index(self, observation):\n",
    "        # because cartpole observations are continuous, we have to bucket them and\n",
    "        # calculate an index for the qtable\n",
    "        position, velocity, theta, theta_velocity = observation\n",
    "\n",
    "        bucketed_position = self._bucket(position, CARTPOLE_POSITION_BUCKETS, CARTPOLE_POSITION_RANGE)\n",
    "        bucketed_velocity = self._bucket(velocity, CARTPOLE_VELOCITY_BUCKETS, CARTPOLE_VELOCITY_RANGE)\n",
    "        bucketed_theta = self._bucket(theta, CARTPOLE_THETA_BUCKETS, CARTPOLE_THETA_RANGE)\n",
    "        bucketed_theta_velocity = self._bucket(theta_velocity, CARTPOLE_THETA_VELOCITY_BUCKETS, CARTPOLE_THETA_VELOCITY_RANGE)\n",
    "\n",
    "        position_index = (bucketed_position - 1) * CARTPOLE_VELOCITY_BUCKETS * CARTPOLE_THETA_BUCKETS * CARTPOLE_THETA_VELOCITY_BUCKETS\n",
    "        velocity_index = (bucketed_velocity - 1) * CARTPOLE_THETA_BUCKETS * CARTPOLE_THETA_VELOCITY_BUCKETS\n",
    "        theta_index = (bucketed_theta - 1) * CARTPOLE_THETA_VELOCITY_BUCKETS\n",
    "        theta_velocity_index = (bucketed_theta_velocity - 1)\n",
    "\n",
    "        index = position_index + velocity_index + theta_index + theta_velocity_index\n",
    "        return index\n",
    "\n",
    "    def _bucket(self, observation, num_buckets, obs_range):\n",
    "        # calculate bucket number\n",
    "        r_min = obs_range[0]\n",
    "        r_max = obs_range[1]\n",
    "        r_range = r_max - r_min\n",
    "        bucket_size = r_range / num_buckets\n",
    "        bucket = math.ceil((observation + r_range / 2) / bucket_size)\n",
    "\n",
    "        # bound\n",
    "        bucket = min(bucket, num_buckets)\n",
    "        bucket = max(bucket, 1)\n",
    "        return bucket\n",
    "\n",
    "    # FROZEN LAKE\n",
    "\n",
    "    def initialize_frozen_lake_q_table(self, env):\n",
    "        self.q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    def frozen_lake_training_action(self, env, observation):\n",
    "        self.previous_observation = observation\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return env.action_space.sample() # explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[observation]) # exploit\n",
    "\n",
    "    def frozen_lake_evaluation_action(self, observation):\n",
    "        return np.argmax(self.q_table[observation])\n",
    "\n",
    "    def frozen_lake_update(self, observation, action, reward):\n",
    "        # updates the previous observation qtable entry with the reward gained,\n",
    "        # uses the maximum/best future option always\n",
    "        old_value = self.q_table[self.previous_observation, action]\n",
    "        next_max = np.max(self.q_table[observation])\n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n",
    "        self.q_table[self.previous_observation, action] = new_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
